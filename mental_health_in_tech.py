# -*- coding: utf-8 -*-
"""Mental Health in Tech

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wKtXso7Fl9av9KsW0Nck__P20zVHXXLc
"""

# Commented out IPython magic to ensure Python compatibility.
# Imports
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive')

survey_2016 = pd.read_excel('/content/drive/MyDrive/cleaned_data.xlsx')
survey_2016.head()

"""# I. Data and Preprocessing (cleaning, missing values, recoding)

I worked on the `2016 OSMI Mental Health in Tech Survey`. It had 1433 rows and 63 columns, lot's of missing data, lot's of not integrated data (gender for example was left an open question), lot's of countries but not consistent in number of respondents etc.

Sumarized, what I did below is:
* rename columns
* sex columns & company size recoded
* removed outliers from age
* missing value listwise deletion (for variables where missing observations were more than half) and simple imputation
* column encoding
* country filtering (remained only with the ones with more than 30 responses)
* create tech column with flag 1/0

### encoding
"""

from sklearn.preprocessing import OneHotEncoder

# 假設 current_mh_disorder 是目標欄位
target_col = 'current_mh_disorder'
not_encode_cols = [target_col]

# 分離資料（除了目標與地理欄位，其餘都編碼）
data_to_encode = survey_2016.drop(columns=not_encode_cols)
data_not_encode = survey_2016[not_encode_cols]  # 保留這些欄位，包括 target

# 找出類別型欄位
categorical_cols = data_to_encode.select_dtypes(include=['object', 'category']).columns

# OneHotEncoding
encoder = OneHotEncoder(sparse_output=False, drop='first')
encoded_array = encoder.fit_transform(data_to_encode[categorical_cols])
encoded_cols = encoder.get_feature_names_out(categorical_cols)
encoded_df = pd.DataFrame(encoded_array, columns=encoded_cols, index=data_to_encode.index)

# 數值欄位保留
numeric_df = data_to_encode.drop(columns=categorical_cols)

# 合併所有資料（數值 + 編碼 + 保留欄位）
final_df = pd.concat([numeric_df, encoded_df, data_not_encode], axis=1)
final_df.head()

from sklearn.preprocessing import LabelEncoder

# 編碼為數字類別
le = LabelEncoder()
final_df['current_mh_disorder'] = le.fit_transform(final_df['current_mh_disorder'])

# 查看對應關係
print(dict(zip(le.classes_, le.transform(le.classes_))))

"""# III. Machine Learning - Making Predictions
## Predicting the "has_current_mental_health_disorder" variable

### 0.Imports
"""

from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import SGDClassifier, LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier, XGBRFClassifier

from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve
from sklearn import preprocessing

"""### 1.Data Preparing"""

# 分割資料
X = final_df.drop(columns=['current_mh_disorder'])
y = final_df['current_mh_disorder']

# Data Validation
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""### 2.Models"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

def model_assess(model, name='Default'):
    model.fit(X_train, y_train)
    preds = model.predict(X_test)

    print('---', name, '---')
    print('Confusion Matrix:\n', confusion_matrix(y_test, preds))
    print('\nClassification Report:\n', classification_report(y_test, preds, digits=4))

    print('Eval Metrics:')
    print('Accuracy:', round(accuracy_score(y_test, preds), 5))
    print('Precision:', round(precision_score(y_test, preds, average='weighted'), 5))
    print('Recall:', round(recall_score(y_test, preds, average='weighted'), 5))
    print('F1 Score:', round(f1_score(y_test, preds, average='weighted'), 5))

# Naive Bayes
nb = GaussianNB()
model_assess(nb, name='Naive Bayes')

# Stochastic Gradient Descent
sgd = SGDClassifier(max_iter=5000, random_state=0)
model_assess(sgd, name='SGD')

# KNN
knn = KNeighborsClassifier(n_neighbors=19)
model_assess(knn, name='KNN')

# Decission trees
tree = DecisionTreeClassifier()
model_assess(tree, 'Decission Trees')

# Random Forest
rforest = RandomForestClassifier(n_estimators=1000, max_depth=10, random_state=0)
model_assess(rforest, 'Random Forest')

# Support Vector Machine
svm = SVC(decision_function_shape="ovo")
model_assess(svm, 'SVM')

# Logistic Regression
lg = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')
model_assess(lg, 'Logistic Regression')

# Neural Nets
nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(150, 10), random_state=1)
model_assess(nn, 'Neural Nets')

# Cross Gradient Booster
xgb = XGBClassifier(n_estimators=1000, learning_rate=0.05)
model_assess(xgb, 'XGBoost')

# Cross Gradient Booster (Random Forest) <=================== BEST
xgbrf = XGBRFClassifier(objective= 'multi:softmax')
model_assess(xgbrf, 'XGBoost RF')

"""### 3.Parameter tunning for Cross Gradient Booster (random Forests)"""

# 定義 XGBoost RF 模型
xgbrf_model = XGBRFClassifier(
    objective='multi:softmax',
    learning_rate=0.01,
    nthread=4,
    use_label_encoder=False,
    eval_metric='mlogloss'
)

# 參數搜尋空間
param_grid = {
    'n_estimators': [300, 600, 1000],
    'max_depth': [1, 3, 5],
    'min_child_weight': [1, 3, 5],
    'subsample': [0.5, 0.6, 0.8],
    'colsample_bytree': [0.5, 0.6, 0.8],
    'scale_pos_weight': [1, 2]
}

# GridSearchCV 訓練
print("正在執行 XGBoost RF 的參數搜尋與訓練...")
grid_search = GridSearchCV(
    estimator=xgbrf_model,
    param_grid=param_grid,
    cv=5,
    n_jobs=-1,
    scoring='accuracy'
)
grid_search.fit(X_train, y_train)

# 輸出結果
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

print("最佳參數:", grid_search.best_params_)
print("交叉驗證最佳分數:", round(grid_search.best_score_, 4))
print("測試集準確率:", round(accuracy_score(y_test, y_pred), 4))
print("\n分類報告:\n", classification_report(y_test, y_pred))
print("混淆矩陣:\n", confusion_matrix(y_test, y_pred))

"""So the best Accuracy is 0.7771.

We notice that value 0 (Maybe) and 2 (yes) have slightly more failed predictions, but values 1 (no MH) have very high accuracies

## Bonus: Feature Importance
"""

!pip install eli5

import eli5
from eli5.sklearn import PermutationImportance

# 用最佳模型來做 permutation importance
perm = PermutationImportance(estimator=best_model, random_state=1)
perm.fit(X_test, y_test)

# 顯示變數重要性（在 notebook 環境會以 HTML 格式顯示）
eli5.show_weights(estimator=perm, feature_names=X_test.columns.tolist())

# Creating new train datasets only with features that have an importance bigger than 0.01
X_train_new = X_train[['past_mh_disorder_Yes', 'interfere_work_untreated_Not applicable to me', 'past_mh_disorder_No', 'interfere_work_treated_Not applicable to me', 'family_history_No', 'interfere_work_untreated_Often', 'mh_diagnosed_prof_Yes']]
X_test_new = X_test[['past_mh_disorder_Yes', 'interfere_work_untreated_Not applicable to me', 'past_mh_disorder_No', 'interfere_work_treated_Not applicable to me', 'family_history_No', 'interfere_work_untreated_Often', 'mh_diagnosed_prof_Yes']]

# Fitting the model to the new train datasets to see if we get a better accuracy
xgbrf_best = XGBRFClassifier(
    objective='multi:softmax',
    colsample_bytree=0.8,
    max_depth=5,
    min_child_weight=1,
    n_estimators=1000,
    scale_pos_weight=1,
    subsample=0.6,
    use_label_encoder=False,
    eval_metric='mlogloss',
    nthread=4
)
# 模型訓練與預測
xgbrf_best.fit(X_train_new, y_train)
predictions = xgbrf_best.predict(X_test_new)

# 評估模型
print("Confusion Matrix:")
print(confusion_matrix(y_test, predictions))
print("Accuracy:", accuracy_score(y_test, predictions))
print("Classification Report:")
print(classification_report(y_test, predictions, target_names=le.classes_))

"""The accuracy is same as the one we got before. In conclusion, it is very good to know that using only 7 feature columns (其實是五個變數：past_mh_disorder, interfere_work_untreated, interfere_work_treated, family_history, mh_diagnosed_prof) is enough to create a very similar model with almost the same performance.

### Plotting feature Importance
"""

from xgboost import plot_tree, plot_importance

fig, (ax1, ax2) = plt.subplots(figsize = (13, 8), ncols=1, nrows=2)
plt.subplots_adjust(left=0.125, right=0.9, bottom=0.1, top = 0.9, wspace=0, hspace = 0.5)

plot_importance(xgbrf_best, importance_type='gain', ax = ax1)
ax1.set_title('Feature Importance by Information Gain', fontsize = 18)
ax1.set_xlabel('Gain')

plot_importance(xgbrf_best, importance_type='weight', ax = ax2)
ax2.set_title('Feature Importance by Weight', fontsize = 18)
ax2.set_xlabel('Weight');